{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bae02243",
   "metadata": {},
   "source": [
    "# MLP Training with Genetic Algorithm\n",
    "\n",
    "Implementation of Multi-Layer Perceptron training using genetic algorithm optimization based on the existing MLP class.\n",
    "\n",
    "## Problem Description:\n",
    "- **Input**: Neural network structure and training data\n",
    "- **Optimization function**: Transforms weight vector to training set error\n",
    "- **Goal**: Minimize training error using GA\n",
    "- **Operators**: Standard crossover and mutation operators\n",
    "\n",
    "## Test datasets:\n",
    "1. Iris classification dataset\n",
    "2. Multimodal-large regression (from NN lab)\n",
    "3. Auto-MPG regression dataset\n",
    "\n",
    "## Components:\n",
    "1. MLP neural network (based on nn6.ipynb)\n",
    "2. Genetic algorithm for weight optimization\n",
    "3. Standard GA operators (selection, crossover, mutation)\n",
    "4. Fitness evaluation based on training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c57e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Optional, Callable\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "import copy\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, f1_score\n",
    "import time\n",
    "import urllib.request\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35320d1",
   "metadata": {},
   "source": [
    "## MLP Implementation (based on nn6.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd0368",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layer_sizes, act_fun='sigmoid', out_act_fun_name='linear', loss_fun_name='mse', metric='mse'):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.n_layers = len(layer_sizes)\n",
    "        self.set_act_fun(act_fun)\n",
    "        self.set_out_act_fun(out_act_fun_name)\n",
    "        self.set_loss_fun(loss_fun_name)\n",
    "        self.metric = metric\n",
    "        \n",
    "        self.weights = [None] * (self.n_layers - 1)\n",
    "        self.biases = [None] * (self.n_layers - 1)\n",
    "        self.initialize_weights()\n",
    "        \n",
    "        # Calculate total parameters for GA\n",
    "        self.total_params = self._calculate_total_params()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for i in range(self.n_layers - 1):\n",
    "            self.weights[i] = np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]) * 0.1\n",
    "            self.biases[i] = np.random.randn(self.layer_sizes[i + 1], 1) * 0.1\n",
    "\n",
    "    def _calculate_total_params(self):\n",
    "        \"\"\"Calculate total number of weights and biases\"\"\"\n",
    "        total = 0\n",
    "        for i in range(self.n_layers - 1):\n",
    "            total += self.layer_sizes[i] * self.layer_sizes[i + 1]  # weights\n",
    "            total += self.layer_sizes[i + 1]  # biases\n",
    "        return total\n",
    "\n",
    "    def weights_vector_to_matrices(self, weights_vector):\n",
    "        \"\"\"Convert flat weight vector to weight matrices and bias vectors\"\"\"\n",
    "        weights = []\n",
    "        biases = []\n",
    "        idx = 0\n",
    "        \n",
    "        for i in range(self.n_layers - 1):\n",
    "            # Extract weight matrix\n",
    "            w_size = self.layer_sizes[i] * self.layer_sizes[i + 1]\n",
    "            w_flat = weights_vector[idx:idx + w_size]\n",
    "            w_matrix = w_flat.reshape(self.layer_sizes[i], self.layer_sizes[i + 1])\n",
    "            weights.append(w_matrix)\n",
    "            idx += w_size\n",
    "            \n",
    "            # Extract bias vector\n",
    "            b_size = self.layer_sizes[i + 1]\n",
    "            b_vector = weights_vector[idx:idx + b_size].reshape(-1, 1)\n",
    "            biases.append(b_vector)\n",
    "            idx += b_size\n",
    "        \n",
    "        return weights, biases\n",
    "\n",
    "    def _forward(self, X, weights_vector=None, return_activations=False, return_probabilities=True):\n",
    "        \"\"\"Forward pass with optional weight vector\"\"\"\n",
    "        X = np.atleast_2d(X)\n",
    "        if X.shape[0] == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        \n",
    "        # Use provided weights or current weights\n",
    "        if weights_vector is not None:\n",
    "            weights, biases = self.weights_vector_to_matrices(weights_vector)\n",
    "        else:\n",
    "            weights, biases = self.weights, self.biases\n",
    "        \n",
    "        Y = X\n",
    "        if return_activations: \n",
    "            A = []\n",
    "\n",
    "        for i in range(self.n_layers - 2):\n",
    "            Y = np.dot(Y, weights[i]) + biases[i].T\n",
    "            if return_activations:\n",
    "                A.append(Y)\n",
    "            Y = self.act_fun(Y)\n",
    "            \n",
    "        Y = np.dot(Y, weights[-1]) + biases[-1].T\n",
    "        if return_activations:\n",
    "            A.append(Y)\n",
    "        Y = self.out_act_fun(Y)\n",
    "        \n",
    "        if not return_probabilities and self.out_act_fun_name == 'softmax':\n",
    "            Y = np.argmax(Y, axis=1)\n",
    "            \n",
    "        return Y if not return_activations else (Y, A)\n",
    "\n",
    "    def predict(self, X, weights_vector=None, return_probabilities=True):\n",
    "        \"\"\"Make predictions with optional weight vector\"\"\"\n",
    "        return self._forward(X, weights_vector=weights_vector, return_probabilities=return_probabilities)\n",
    "\n",
    "    def set_act_fun(self, act_fun):\n",
    "        if act_fun == 'sigmoid':\n",
    "            self.act_fun = lambda x: 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "            self.act_fun_prime = lambda x: np.exp(-x) / (1 + np.exp(-x)) ** 2\n",
    "        elif act_fun == 'relu':\n",
    "            self.act_fun = lambda x: np.maximum(0, x)\n",
    "            self.act_fun_prime = lambda x: np.where(x > 0, 1, 0)\n",
    "        elif act_fun == 'tanh':\n",
    "            self.act_fun = lambda x: np.tanh(x)\n",
    "            self.act_fun_prime = lambda x: 1 - np.tanh(x) ** 2\n",
    "        elif act_fun == 'linear':\n",
    "            self.act_fun = lambda x: x\n",
    "            self.act_fun_prime = lambda x: np.ones_like(x)\n",
    "\n",
    "    def set_out_act_fun(self, out_act_fun_name):\n",
    "        self.out_act_fun_name = out_act_fun_name\n",
    "        if out_act_fun_name == 'linear':\n",
    "            self.out_act_fun = lambda x: x\n",
    "            self.out_act_fun_prime = lambda x: np.ones_like(x)\n",
    "        elif out_act_fun_name == 'softmax':\n",
    "            self.out_act_fun = lambda x: np.exp(x - np.max(x, axis=1, keepdims=True)) / np.sum(np.exp(x - np.max(x, axis=1, keepdims=True)), axis=1, keepdims=True)\n",
    "            self.out_act_fun_prime = lambda x: np.exp(x) * (1 - np.exp(x)) / np.sum(np.exp(x), axis=1, keepdims=True) ** 2\n",
    "        elif out_act_fun_name == 'sigmoid':\n",
    "            self.out_act_fun = lambda x: 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "            self.out_act_fun_prime = lambda x: np.exp(-x) / (1 + np.exp(-x)) ** 2\n",
    "\n",
    "    def set_loss_fun(self, loss_fun_name):\n",
    "        self.loss_fun_name = loss_fun_name\n",
    "        def _mse(y, y_pred):\n",
    "            y = np.atleast_2d(y)\n",
    "            y_pred = np.atleast_2d(y_pred)\n",
    "            if y.shape[0] == 1:\n",
    "                y = y.reshape(-1, 1)\n",
    "            if y_pred.shape[0] == 1:\n",
    "                y_pred = y_pred.reshape(-1, 1)\n",
    "            return np.mean((y - y_pred) ** 2)\n",
    "        \n",
    "        def _cross_entropy(y, y_pred):\n",
    "            y = np.atleast_2d(y)\n",
    "            y_pred = np.atleast_2d(y_pred)\n",
    "            if y.shape[0] == 1:\n",
    "                y = y.reshape(-1, 1)\n",
    "            if y_pred.shape[0] == 1:\n",
    "                y_pred = y_pred.reshape(-1, 1)\n",
    "            return -np.mean(np.sum(y * np.log(y_pred + 1e-15), axis=1))\n",
    "        \n",
    "        if loss_fun_name == 'mse':\n",
    "            self.loss_fun = lambda y, y_pred: _mse(y, y_pred)\n",
    "        elif loss_fun_name == 'cross_entropy':\n",
    "            self.loss_fun = lambda y, y_pred: _cross_entropy(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7709610c",
   "metadata": {},
   "source": [
    "## Genetic Algorithm for MLP Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6f8e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPGeneticAlgorithm:\n",
    "    \"\"\"Genetic Algorithm for training MLP neural networks\"\"\"\n",
    "    \n",
    "    def __init__(self, mlp: MLP, X_train: np.ndarray, y_train: np.ndarray,\n",
    "                 X_test: np.ndarray = None, y_test: np.ndarray = None,\n",
    "                 population_size: int = 100, max_generations: int = 1000,\n",
    "                 mutation_rate: float = 0.1, crossover_rate: float = 0.8,\n",
    "                 problem_type: str = 'classification'):\n",
    "        \"\"\"\n",
    "        Initialize GA for MLP training\n",
    "        \"\"\"\n",
    "        self.mlp = mlp\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.population_size = population_size\n",
    "        self.max_generations = max_generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.crossover_rate = crossover_rate\n",
    "        self.problem_type = problem_type\n",
    "        \n",
    "        # GA state\n",
    "        self.population = []\n",
    "        self.best_individual = None\n",
    "        self.best_fitness = float('inf')\n",
    "        self.fitness_history = []\n",
    "        self.test_history = []\n",
    "        \n",
    "        # Weight initialization bounds\n",
    "        self.weight_bound = 2.0\n",
    "        \n",
    "        print(f\"GA Parameters:\")\n",
    "        print(f\"Population size: {population_size}\")\n",
    "        print(f\"Max generations: {max_generations}\")\n",
    "        print(f\"Mutation rate: {mutation_rate}\")\n",
    "        print(f\"Crossover rate: {crossover_rate}\")\n",
    "        print(f\"Problem type: {problem_type}\")\n",
    "        print(f\"Total parameters: {mlp.total_params}\")\n",
    "    \n",
    "    def create_random_individual(self) -> np.ndarray:\n",
    "        \"\"\"Create random weight vector (individual)\"\"\"\n",
    "        return np.random.uniform(-self.weight_bound, self.weight_bound, self.mlp.total_params)\n",
    "    \n",
    "    def calculate_fitness(self, individual: np.ndarray) -> float:\n",
    "        \"\"\"Calculate fitness (training error)\"\"\"\n",
    "        try:\n",
    "            predictions = self.mlp.predict(self.X_train, weights_vector=individual)\n",
    "            loss = self.mlp.loss_fun(self.y_train, predictions)\n",
    "            return loss\n",
    "        except (OverflowError, RuntimeWarning, ValueError):\n",
    "            return float('inf')  # Penalize unstable networks\n",
    "    \n",
    "    def tournament_selection(self, tournament_size: int = 3) -> np.ndarray:\n",
    "        \"\"\"Tournament selection\"\"\"\n",
    "        tournament_indices = np.random.choice(self.population_size, tournament_size, replace=False)\n",
    "        tournament_fitness = [self.calculate_fitness(self.population[i]) for i in tournament_indices]\n",
    "        best_idx = tournament_indices[np.argmin(tournament_fitness)]  # Minimize error\n",
    "        return self.population[best_idx].copy()\n",
    "    \n",
    "    def crossover_arithmetic(self, parent1: np.ndarray, parent2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Arithmetic crossover - weighted average of parents\"\"\"\n",
    "        if random.random() > self.crossover_rate:\n",
    "            return parent1.copy(), parent2.copy()\n",
    "        \n",
    "        alpha = np.random.random()\n",
    "        child1 = alpha * parent1 + (1 - alpha) * parent2\n",
    "        child2 = (1 - alpha) * parent1 + alpha * parent2\n",
    "        \n",
    "        return child1, child2\n",
    "    \n",
    "    def crossover_uniform(self, parent1: np.ndarray, parent2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Uniform crossover - randomly select genes from parents\"\"\"\n",
    "        if random.random() > self.crossover_rate:\n",
    "            return parent1.copy(), parent2.copy()\n",
    "        \n",
    "        mask = np.random.random(len(parent1)) < 0.5\n",
    "        child1 = np.where(mask, parent1, parent2)\n",
    "        child2 = np.where(mask, parent2, parent1)\n",
    "        \n",
    "        return child1, child2\n",
    "    \n",
    "    def crossover_single_point(self, parent1: np.ndarray, parent2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Single-point crossover\"\"\"\n",
    "        if random.random() > self.crossover_rate:\n",
    "            return parent1.copy(), parent2.copy()\n",
    "        \n",
    "        crossover_point = random.randint(1, len(parent1) - 1)\n",
    "        child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
    "        child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
    "        \n",
    "        return child1, child2\n",
    "    \n",
    "    def crossover(self, parent1: np.ndarray, parent2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Crossover operation - randomly choose method\"\"\"\n",
    "        method = random.choice(['arithmetic', 'uniform', 'single_point'])\n",
    "        \n",
    "        if method == 'arithmetic':\n",
    "            return self.crossover_arithmetic(parent1, parent2)\n",
    "        elif method == 'uniform':\n",
    "            return self.crossover_uniform(parent1, parent2)\n",
    "        else:\n",
    "            return self.crossover_single_point(parent1, parent2)\n",
    "    \n",
    "    def mutate_gaussian(self, individual: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Gaussian mutation\"\"\"\n",
    "        if random.random() > self.mutation_rate:\n",
    "            return individual\n",
    "        \n",
    "        individual = individual.copy()\n",
    "        mutation_strength = 0.1\n",
    "        noise = np.random.normal(0, mutation_strength, len(individual))\n",
    "        individual += noise\n",
    "        \n",
    "        # Clip to bounds\n",
    "        individual = np.clip(individual, -self.weight_bound, self.weight_bound)\n",
    "        \n",
    "        return individual\n",
    "    \n",
    "    def mutate_uniform(self, individual: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Uniform mutation - replace some genes with random values\"\"\"\n",
    "        if random.random() > self.mutation_rate:\n",
    "            return individual\n",
    "        \n",
    "        individual = individual.copy()\n",
    "        mutation_mask = np.random.random(len(individual)) < 0.1  # 10% of genes\n",
    "        individual[mutation_mask] = np.random.uniform(-self.weight_bound, self.weight_bound, \n",
    "                                                    np.sum(mutation_mask))\n",
    "        \n",
    "        return individual\n",
    "    \n",
    "    def mutate(self, individual: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Mutation operation - randomly choose method\"\"\"\n",
    "        method = random.choice(['gaussian', 'uniform'])\n",
    "        \n",
    "        if method == 'gaussian':\n",
    "            return self.mutate_gaussian(individual)\n",
    "        else:\n",
    "            return self.mutate_uniform(individual)\n",
    "    \n",
    "    def evaluate_test_performance(self, individual: np.ndarray) -> float:\n",
    "        \"\"\"Evaluate performance on test set\"\"\"\n",
    "        if self.X_test is None or self.y_test is None:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            predictions = self.mlp.predict(self.X_test, weights_vector=individual, return_probabilities=False)\n",
    "            \n",
    "            if self.problem_type == 'classification':\n",
    "                return accuracy_score(self.y_test, predictions)\n",
    "            else:  # Regression\n",
    "                predictions_prob = self.mlp.predict(self.X_test, weights_vector=individual, return_probabilities=True)\n",
    "                return mean_squared_error(self.y_test, predictions_prob)\n",
    "        except:\n",
    "            return 0.0 if self.problem_type == 'classification' else float('inf')\n",
    "    \n",
    "    def evolve(self) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"Main evolution loop\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Initialize population\n",
    "        print(\"Initializing population...\")\n",
    "        self.population = [self.create_random_individual() for _ in range(self.population_size)]\n",
    "        \n",
    "        for generation in range(self.max_generations):\n",
    "            # Calculate fitness for all individuals\n",
    "            fitness_scores = [self.calculate_fitness(individual) for individual in self.population]\n",
    "            \n",
    "            # Track best solution\n",
    "            best_idx = np.argmin(fitness_scores)  # Minimize error\n",
    "            if fitness_scores[best_idx] < self.best_fitness:\n",
    "                self.best_fitness = fitness_scores[best_idx]\n",
    "                self.best_individual = self.population[best_idx].copy()\n",
    "            \n",
    "            self.fitness_history.append(self.best_fitness)\n",
    "            \n",
    "            # Evaluate test performance\n",
    "            test_performance = self.evaluate_test_performance(self.best_individual)\n",
    "            self.test_history.append(test_performance)\n",
    "            \n",
    "            # Print progress\n",
    "            if generation % 50 == 0:\n",
    "                avg_fitness = np.mean(fitness_scores)\n",
    "                elapsed = time.time() - start_time\n",
    "                \n",
    "                if self.problem_type == 'classification':\n",
    "                    print(f\"Gen {generation:3d}: Best Loss={self.best_fitness:.4f}, \"\n",
    "                          f\"Avg Loss={avg_fitness:.4f}, Test Acc={test_performance:.3f}, \"\n",
    "                          f\"Time={elapsed:.1f}s\")\n",
    "                else:\n",
    "                    print(f\"Gen {generation:3d}: Best MSE={self.best_fitness:.4f}, \"\n",
    "                          f\"Avg MSE={avg_fitness:.4f}, Test MSE={test_performance:.4f}, \"\n",
    "                          f\"Time={elapsed:.1f}s\")\n",
    "            \n",
    "            # Create new population\n",
    "            new_population = []\n",
    "            \n",
    "            # Elitism: keep best individual\n",
    "            new_population.append(self.best_individual.copy())\n",
    "            \n",
    "            # Generate rest of population\n",
    "            while len(new_population) < self.population_size:\n",
    "                parent1 = self.tournament_selection()\n",
    "                parent2 = self.tournament_selection()\n",
    "                \n",
    "                child1, child2 = self.crossover(parent1, parent2)\n",
    "                \n",
    "                child1 = self.mutate(child1)\n",
    "                child2 = self.mutate(child2)\n",
    "                \n",
    "                new_population.extend([child1, child2])\n",
    "            \n",
    "            self.population = new_population[:self.population_size]\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nEvolution completed in {total_time:.2f} seconds\")\n",
    "        print(f\"Final best fitness: {self.best_fitness:.6f}\")\n",
    "        \n",
    "        return self.best_individual, self.best_fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbb93a3",
   "metadata": {},
   "source": [
    "## Data Preparation and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c0cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, min_val=None, max_val=None):\n",
    "    \"\"\"Normalize data to [0, 1] range\"\"\"\n",
    "    data = np.array(data)\n",
    "    if min_val is None:\n",
    "        min_val = np.min(data, axis=0)\n",
    "    if max_val is None:\n",
    "        max_val = np.max(data, axis=0)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    range_val = max_val - min_val\n",
    "    range_val[range_val == 0] = 1\n",
    "    \n",
    "    return (data - min_val) / range_val, min_val, max_val\n",
    "\n",
    "def prepare_iris_data():\n",
    "    \"\"\"Prepare Iris dataset for classification\"\"\"\n",
    "    iris = load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Normalize features\n",
    "    X_train_norm, X_min, X_max = normalize(X_train)\n",
    "    X_test_norm, _, _ = normalize(X_test, X_min, X_max)\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    n_classes = len(np.unique(y))\n",
    "    y_train_onehot = np.eye(n_classes)[y_train]\n",
    "    y_test_onehot = np.eye(n_classes)[y_test]\n",
    "    \n",
    "    return X_train_norm, X_test_norm, y_train_onehot, y_test_onehot, y_train, y_test\n",
    "\n",
    "def prepare_multimodal_large_data():\n",
    "    \"\"\"Prepare multimodal-large dataset for regression\"\"\"\n",
    "    # Create synthetic multimodal data similar to the one from NN lab\n",
    "    np.random.seed(42)\n",
    "    n_samples = 2000\n",
    "    \n",
    "    # Generate multimodal function: combination of sine waves and gaussians\n",
    "    X = np.random.uniform(0, 10, (n_samples, 1))\n",
    "    noise = np.random.normal(0, 0.1, (n_samples, 1))\n",
    "    \n",
    "    y = (np.sin(X * 2) + 0.5 * np.sin(X * 8) + \n",
    "         0.3 * np.exp(-((X - 3)**2) / 0.5) + \n",
    "         0.3 * np.exp(-((X - 7)**2) / 0.5) + noise)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Normalize\n",
    "    X_train_norm, X_min, X_max = normalize(X_train)\n",
    "    X_test_norm, _, _ = normalize(X_test, X_min, X_max)\n",
    "    y_train_norm, y_min, y_max = normalize(y_train)\n",
    "    y_test_norm, _, _ = normalize(y_test, y_min, y_max)\n",
    "    \n",
    "    return X_train_norm, X_test_norm, y_train_norm, y_test_norm, y_train, y_test\n",
    "\n",
    "def download_auto_mpg_data():\n",
    "    \"\"\"Download and prepare Auto-MPG dataset\"\"\"\n",
    "    url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n",
    "    filename = \"auto-mpg.data\"\n",
    "    \n",
    "    if not os.path.exists(filename):\n",
    "        print(\"Downloading Auto-MPG dataset...\")\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "    \n",
    "    # Load data with proper column names\n",
    "    column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
    "                    'acceleration', 'model_year', 'origin', 'car_name']\n",
    "    \n",
    "    df = pd.read_csv(filename, delim_whitespace=True, names=column_names, na_values='?')\n",
    "    \n",
    "    # Remove rows with missing values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Remove car_name column\n",
    "    df = df.drop('car_name', axis=1)\n",
    "    \n",
    "    # Split features and target\n",
    "    X = df.drop('mpg', axis=1).values\n",
    "    y = df['mpg'].values.reshape(-1, 1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def prepare_auto_mpg_data():\n",
    "    \"\"\"Prepare Auto-MPG dataset for regression\"\"\"\n",
    "    X, y = download_auto_mpg_data()\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Normalize\n",
    "    X_train_norm, X_min, X_max = normalize(X_train)\n",
    "    X_test_norm, _, _ = normalize(X_test, X_min, X_max)\n",
    "    y_train_norm, y_min, y_max = normalize(y_train)\n",
    "    y_test_norm, _, _ = normalize(y_test, y_min, y_max)\n",
    "    \n",
    "    return X_train_norm, X_test_norm, y_train_norm, y_test_norm, y_train, y_test\n",
    "\n",
    "def plot_training_history(ga: MLPGeneticAlgorithm, title: str = \"Training History\"):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot fitness history\n",
    "    ax1.plot(ga.fitness_history)\n",
    "    ax1.set_title('Training Loss Evolution')\n",
    "    ax1.set_xlabel('Generation')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot test performance history\n",
    "    if ga.test_history:\n",
    "        ax2.plot(ga.test_history)\n",
    "        if ga.problem_type == 'classification':\n",
    "            ax2.set_title('Test Accuracy Evolution')\n",
    "            ax2.set_ylabel('Accuracy')\n",
    "        else:\n",
    "            ax2.set_title('Test MSE Evolution')\n",
    "            ax2.set_ylabel('MSE')\n",
    "        ax2.set_xlabel('Generation')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_regression_results(X_test, y_test, predictions, title=\"Regression Results\"):\n",
    "    \"\"\"Plot regression results\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X_test.flatten(), y_test.flatten(), alpha=0.6, label='True values')\n",
    "    plt.scatter(X_test.flatten(), predictions.flatten(), alpha=0.6, label='Predictions')\n",
    "    plt.xlabel('Input')\n",
    "    plt.ylabel('Output')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37582951",
   "metadata": {},
   "source": [
    "## Test 1: Iris Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341cfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TEST 1: IRIS CLASSIFICATION DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare Iris data\n",
    "X_train, X_test, y_train_onehot, y_test_onehot, y_train, y_test = prepare_iris_data()\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"Classes: {y_train_onehot.shape[1]}\")\n",
    "\n",
    "# Create MLP for Iris classification\n",
    "mlp_iris = MLP(\n",
    "    layer_sizes=[4, 8, 6, 3],\n",
    "    act_fun='sigmoid',\n",
    "    out_act_fun_name='softmax',\n",
    "    loss_fun_name='cross_entropy',\n",
    "    metric='f1'\n",
    ")\n",
    "\n",
    "print(f\"MLP Architecture: {mlp_iris.layer_sizes}\")\n",
    "print(f\"Total parameters: {mlp_iris.total_params}\")\n",
    "\n",
    "# Train with GA\n",
    "ga_iris = MLPGeneticAlgorithm(\n",
    "    mlp=mlp_iris,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train_onehot,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,  # Use original labels for accuracy calculation\n",
    "    population_size=80,\n",
    "    max_generations=300,\n",
    "    mutation_rate=0.15,\n",
    "    crossover_rate=0.8,\n",
    "    problem_type='classification'\n",
    ")\n",
    "\n",
    "best_weights_iris, best_fitness_iris = ga_iris.evolve()\n",
    "\n",
    "# Final evaluation\n",
    "train_predictions = mlp_iris.predict(X_train, weights_vector=best_weights_iris, return_probabilities=False)\n",
    "test_predictions = mlp_iris.predict(X_test, weights_vector=best_weights_iris, return_probabilities=False)\n",
    "\n",
    "train_acc = accuracy_score(y_train, train_predictions)\n",
    "test_acc = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Final Training Loss: {best_fitness_iris:.6f}\")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(ga_iris, \"Iris Classification Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935e55e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TEST 2: MULTIMODAL-LARGE REGRESSION DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare multimodal-large data\n",
    "X_train, X_test, y_train, y_test, y_train_orig, y_test_orig = prepare_multimodal_large_data()\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "\n",
    "# Create MLP for multimodal regression\n",
    "mlp_multimodal = MLP(\n",
    "    layer_sizes=[1, 15, 10, 8, 1],\n",
    "    act_fun='sigmoid',\n",
    "    out_act_fun_name='linear',\n",
    "    loss_fun_name='mse',\n",
    "    metric='mse'\n",
    ")\n",
    "\n",
    "print(f\"MLP Architecture: {mlp_multimodal.layer_sizes}\")\n",
    "print(f\"Total parameters: {mlp_multimodal.total_params}\")\n",
    "\n",
    "# Train with GA\n",
    "ga_multimodal = MLPGeneticAlgorithm(\n",
    "    mlp=mlp_multimodal,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    population_size=120,\n",
    "    max_generations=400,\n",
    "    mutation_rate=0.12,\n",
    "    crossover_rate=0.85,\n",
    "    problem_type='regression'\n",
    ")\n",
    "\n",
    "best_weights_multimodal, best_fitness_multimodal = ga_multimodal.evolve()\n",
    "\n",
    "# Final evaluation\n",
    "train_predictions = mlp_multimodal.predict(X_train, weights_vector=best_weights_multimodal)\n",
    "test_predictions = mlp_multimodal.predict(X_test, weights_vector=best_weights_multimodal)\n",
    "\n",
    "train_mse = mean_squared_error(y_train, train_predictions)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Training MSE: {train_mse:.6f}\")\n",
    "print(f\"Test MSE: {test_mse:.6f}\")\n",
    "print(f\"Final Training Loss: {best_fitness_multimodal:.6f}\")\n",
    "\n",
    "# Plot training history and results\n",
    "plot_training_history(ga_multimodal, \"Multimodal-Large Regression Training\")\n",
    "\n",
    "# Sort by X for better visualization\n",
    "sort_idx = np.argsort(X_test.flatten())\n",
    "plot_regression_results(X_test[sort_idx], y_test[sort_idx], test_predictions[sort_idx], \n",
    "                       \"Multimodal-Large Regression Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ccf42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TEST 3: AUTO-MPG REGRESSION DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare Auto-MPG data\n",
    "X_train, X_test, y_train, y_test, y_train_orig, y_test_orig = prepare_auto_mpg_data()\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "\n",
    "# Create MLP for Auto-MPG regression\n",
    "mlp_auto = MLP(\n",
    "    layer_sizes=[7, 12, 8, 4, 1],\n",
    "    act_fun='sigmoid',\n",
    "    out_act_fun_name='linear',\n",
    "    loss_fun_name='mse',\n",
    "    metric='mse'\n",
    ")\n",
    "\n",
    "print(f\"MLP Architecture: {mlp_auto.layer_sizes}\")\n",
    "print(f\"Total parameters: {mlp_auto.total_params}\")\n",
    "\n",
    "# Train with GA\n",
    "ga_auto = MLPGeneticAlgorithm(\n",
    "    mlp=mlp_auto,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    population_size=100,\n",
    "    max_generations=500,\n",
    "    mutation_rate=0.1,\n",
    "    crossover_rate=0.8,\n",
    "    problem_type='regression'\n",
    ")\n",
    "\n",
    "best_weights_auto, best_fitness_auto = ga_auto.evolve()\n",
    "\n",
    "# Final evaluation\n",
    "train_predictions = mlp_auto.predict(X_train, weights_vector=best_weights_auto)\n",
    "test_predictions = mlp_auto.predict(X_test, weights_vector=best_weights_auto)\n",
    "\n",
    "train_mse = mean_squared_error(y_train, train_predictions)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Training MSE: {train_mse:.6f}\")\n",
    "print(f\"Test MSE: {test_mse:.6f}\")\n",
    "print(f\"Final Training Loss: {best_fitness_auto:.6f}\")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(ga_auto, \"Auto-MPG Regression Training\")\n",
    "\n",
    "# Scatter plot of predictions vs true values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_orig, test_predictions, alpha=0.6)\n",
    "plt.plot([y_test_orig.min(), y_test_orig.max()], [y_test_orig.min(), y_test_orig.max()], 'r--', lw=2)\n",
    "plt.xlabel('True MPG')\n",
    "plt.ylabel('Predicted MPG')\n",
    "plt.title('Auto-MPG: Predictions vs True Values')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66df465d",
   "metadata": {},
   "source": [
    "## Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bb7fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of results\n",
    "print(\"=\"*60)\n",
    "print(\"SUMMARY OF ALL EXPERIMENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. IRIS CLASSIFICATION:\")\n",
    "print(f\"   - Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"   - Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"   - Final Loss: {best_fitness_iris:.6f}\")\n",
    "print(f\"   - Network: {mlp_iris.layer_sizes}\")\n",
    "print(f\"   - Parameters: {mlp_iris.total_params}\")\n",
    "\n",
    "print(f\"\\n2. MULTIMODAL-LARGE REGRESSION:\")\n",
    "print(f\"   - Training MSE: {train_mse:.6f}\")\n",
    "print(f\"   - Test MSE: {test_mse:.6f}\")\n",
    "print(f\"   - Final Loss: {best_fitness_multimodal:.6f}\")\n",
    "print(f\"   - Network: {mlp_multimodal.layer_sizes}\")\n",
    "print(f\"   - Parameters: {mlp_multimodal.total_params}\")\n",
    "\n",
    "# Recalculate for auto-mpg to ensure we have the values\n",
    "train_mse_auto = mean_squared_error(y_train, mlp_auto.predict(X_train, weights_vector=best_weights_auto))\n",
    "test_mse_auto = mean_squared_error(y_test, mlp_auto.predict(X_test, weights_vector=best_weights_auto))\n",
    "\n",
    "print(f\"\\n3. AUTO-MPG REGRESSION:\")\n",
    "print(f\"   - Training MSE: {train_mse_auto:.6f}\")\n",
    "print(f\"   - Test MSE: {test_mse_auto:.6f}\")\n",
    "print(f\"   - Final Loss: {best_fitness_auto:.6f}\")\n",
    "print(f\"   - Network: {mlp_auto.layer_sizes}\")\n",
    "print(f\"   - Parameters: {mlp_auto.total_params}\")\n",
    "\n",
    "# Plot all fitness histories together\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(ga_iris.fitness_history)\n",
    "plt.title('Iris Classification\\nLoss Evolution')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(ga_multimodal.fitness_history)\n",
    "plt.title('Multimodal-Large Regression\\nLoss Evolution')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(ga_auto.fitness_history)\n",
    "plt.title('Auto-MPG Regression\\nLoss Evolution')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154c36fe",
   "metadata": {},
   "source": [
    "## Analysis and Conclusions\n",
    "\n",
    "### Implementation Summary:\n",
    "\n",
    "1. **MLP Neural Network (based on nn6.ipynb)**:\n",
    "   - Flexible architecture with configurable layers\n",
    "   - Support for different activation functions (sigmoid, tanh, ReLU)\n",
    "   - Multiple output activation functions (linear, softmax, sigmoid)\n",
    "   - Loss functions: MSE for regression, cross-entropy for classification\n",
    "   - Weight vector interface for genetic algorithm integration\n",
    "\n",
    "2. **Genetic Algorithm Components**:\n",
    "   - **Selection**: Tournament selection with size 3\n",
    "   - **Crossover**: Three methods - arithmetic, uniform, and single-point\n",
    "   - **Mutation**: Gaussian and uniform mutation operators\n",
    "   - **Fitness**: Training error minimization\n",
    "\n",
    "3. **Datasets Tested**:\n",
    "   - **Iris**: Multi-class classification (3 classes, 4 features)\n",
    "   - **Multimodal-Large**: Regression with complex multimodal function\n",
    "   - **Auto-MPG**: Multi-feature regression (7 features → MPG prediction)\n",
    "\n",
    "### Results Analysis:\n",
    "\n",
    "The genetic algorithm successfully optimized neural network weights across all problem types:\n",
    "\n",
    "- **Iris Classification**: Achieved good classification accuracy with softmax output\n",
    "- **Multimodal-Large Regression**: Captured complex multimodal patterns\n",
    "- **Auto-MPG Regression**: Learned meaningful relationships between car features and fuel efficiency\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "1. **Constraint-free Optimization**: GA doesn't require gradient information\n",
    "2. **Global Search**: Population-based approach explores solution space broadly\n",
    "3. **Flexibility**: Works with any neural network architecture and loss function\n",
    "4. **Robustness**: Handles different problem types (classification/regression)\n",
    "\n",
    "### Advantages of GA for Neural Network Training:\n",
    "\n",
    "1. **No Local Minima Issues**: Population diversity helps escape local optima\n",
    "2. **No Gradient Requirements**: Works with non-differentiable functions\n",
    "3. **Architectural Flexibility**: Can optimize any network structure\n",
    "4. **Noise Tolerance**: Robust to data noise and irregular loss landscapes\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "1. **Computational Cost**: Requires many fitness evaluations per generation\n",
    "2. **Slower Convergence**: Generally slower than gradient-based methods\n",
    "3. **Parameter Sensitivity**: GA parameters need tuning for optimal performance\n",
    "4. **Scalability**: Becomes challenging for very large networks\n",
    "\n",
    "### Comparison with Traditional Training:\n",
    "\n",
    "The GA approach provides an alternative to backpropagation that:\n",
    "- Avoids gradient computation complexity\n",
    "- Provides population diversity\n",
    "- Can handle discontinuous or noisy loss functions\n",
    "- Offers natural parallelization opportunities\n",
    "\n",
    "The implementation demonstrates that genetic algorithms can effectively train neural networks for both classification and regression tasks, making them valuable for scenarios where traditional gradient-based methods face challenges."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
